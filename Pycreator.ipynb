{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jonipeloni/musicgeneration/blob/main/transformer_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"jaypelone@gmx.com\"\n",
        "!git config --global user.name \"Jonipeloni\"\n"
      ],
      "metadata": {
        "id": "bdKOL2445E-4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jonipeloni/musicgeneration.git\n",
        "%cd yourrepository"
      ],
      "metadata": {
        "id": "3PM-vNJo5HG8",
        "outputId": "a2ce9ca1-9f2b-4739-9e1f-5091bb7f8699",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'musicgeneration'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 22 (delta 10), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (22/22), 1.90 MiB | 10.33 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "[Errno 2] No such file or directory: 'yourrepository'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "AXmCdHHq5KuI",
        "outputId": "7f8b76ee-d00e-4aa3-cc47-b0e5363b8037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/musicgeneration"
      ],
      "metadata": {
        "id": "l5QpAWQF52im",
        "outputId": "7a45e40d-964a-4c89-d320-cbfa50942712",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/musicgeneration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3hYoI-NJQO-o",
        "outputId": "8ad43df9-b485-4c63-e038-d37060a5c6ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing transformer_architecture.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile transformer_architecture.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# from keras_mlp.layers import ReversibleEmbedding\n",
        "\n",
        "class PosEncode(tf.keras.layers.Layer):\n",
        "    def __init__(self,dims,  max_sep_len,):\n",
        "        super(PosEncode, self).__init__()\n",
        "        self.max_sep_len = max_sep_len\n",
        "        self.pos_enc = self._pos_enc(max_sep_len, dims)\n",
        "\n",
        "    #Add the Embeddings and the Encodings\n",
        "\n",
        "    def _pos_enc(self, length, dims):\n",
        "        pos = np.arange(length)[:, np.newaxis]\n",
        "        j = np.arange(dims)[np.newaxis, :]\n",
        "        #distribute the angles according to the formula for Positional Encodings\n",
        "        angle_rates = 1 / np.power(10000, (2 * j)/ dims)\n",
        "        angle_rads = pos * angle_rates\n",
        "        #even coordinates get sin, odd coordinates get cos\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        return tf.slice(self.pos_enc, [0, 0, 0], [-1, seq_len, -1])\n",
        "\n",
        "class InverseEmbedding(tf.keras.layers.Embedding):\n",
        "\n",
        "    def __init__(self, voc_size, dims,  **kwargs):\n",
        "        super(InverseEmbedding, self).__init__(voc_size, dims, **kwargs)\n",
        "\n",
        "    def call(self, x, inverse = False, training = True):\n",
        "        if not inverse:\n",
        "            return super().call(x)\n",
        "\n",
        "        trans = tf.transpose(tf.convert_to_tensor(self.embeddings))\n",
        "        return tf.matmul(x, trans)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dims, num_heads, attn_size, dropout_prob):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Initialize the parameters\n",
        "        self.num_heads = num_heads\n",
        "        self.attn_size = attn_size\n",
        "        self.dims = dims\n",
        "\n",
        "        # Layers for QKV projection and output projection\n",
        "        self.qkv_projection = tf.keras.layers.Dense(num_heads * attn_size * 3, use_bias=False)\n",
        "        self.output_projection = tf.keras.layers.Dense(dims)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_prob)\n",
        "\n",
        "    def call(self, qs, mask=None):\n",
        "        # Compute batch size and sequence length from inputs\n",
        "        batch_size = tf.shape(qs)[0]\n",
        "        seq_length = tf.shape(qs)[1]\n",
        "\n",
        "        # Project queries, keys, and values\n",
        "        qkv = self.qkv_projection(qs)\n",
        "        qkv = tf.reshape(qkv, [batch_size, seq_length, self.num_heads * 3, self.attn_size])\n",
        "        qs, ks, vs = tf.split(qkv, 3, axis=2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        qs = tf.transpose(qs, [0, 2, 1, 3])\n",
        "        ks = tf.transpose(ks, [0, 2, 3, 1])\n",
        "        attn_product = tf.matmul(qs, ks) / tf.math.sqrt(tf.cast(self.attn_size, tf.float32))\n",
        "\n",
        "        # Apply mask, if provided\n",
        "        if mask is not None:\n",
        "            mask = tf.expand_dims(mask, 1)\n",
        "            mask = tf.expand_dims(mask, 2)\n",
        "            mask = tf.broadcast_to(mask, [batch_size, self.num_heads, seq_length, seq_length])\n",
        "            attn_product = tf.where(mask == 0, tf.fill(tf.shape(attn_product), -1e9), attn_product)\n",
        "\n",
        "        # Softmax and dropout\n",
        "        scores = tf.nn.softmax(attn_product, axis=-1)\n",
        "        scores = self.dropout(scores)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        vs = tf.transpose(vs, [0, 2, 1, 3])\n",
        "        res = tf.matmul(scores, vs)\n",
        "\n",
        "        # Reshape and project to output size\n",
        "        res = tf.reshape(tf.transpose(res, [0, 2, 1, 3]), [batch_size, -1, self.num_heads * self.attn_size])\n",
        "        output = self.output_projection(res)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SubLayerLogic(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dropout_rate):\n",
        "        super(SubLayerLogic, self).__init__()\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def calls(self, x, sublayer):\n",
        "\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class SelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate):\n",
        "        super(SelfAttentionBlock, self).__init__()\n",
        "        self.ffn = tf.keras.layers.Dense(d_model, \"silu\")\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads,d_model//num_heads, dropout_rate)\n",
        "        self.sub1 = SubLayerLogic(d_model, dropout_rate)\n",
        "        self.sub2 = SubLayerLogic(d_model, dropout_rate)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, mask):\n",
        "        x = self.sub1(x, lambda inputs : self.attention(inputs, mask))\n",
        "        return self.sub2(x, self.attention)\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, voc_size, dims, maxseqlen, num_heads, dropout_rate, n_blocks):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.posemb = PosEncode(dims, maxseqlen)\n",
        "        self.embedding = InverseEmbedding(voc_size, dims)\n",
        "        self.blocks = [\n",
        "            SelfAttentionBlock(dims, num_heads, dropout_rate) for _ in range(n_blocks)\n",
        "        ]\n",
        "\n",
        "    def encode(self, x):\n",
        "\n",
        "        return self.posemb(x) + self.embedding(x, inverse=False)\n",
        "\n",
        "    def gen_mask(self, seq_len):\n",
        "        return tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)[None,None,:]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, maks = None):\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        x = self.encode(x)\n",
        "\n",
        "        if mask is None:\n",
        "          mask = self.gen_mask(seq_len)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        return self.embedding(x, inverse = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add transformer_architecture.py\n",
        "!git commit -m \"Add new Python module\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "_Zga_cj76Au0",
        "outputId": "7d47a1ec-3c85-4ee6-80cd-7073ee23e020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GITHUB_TOKEN'] = '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'\n",
        "!git config --global credential.helper store\n"
      ],
      "metadata": {
        "id": "bdIIefZu8JOp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://Jonipeloni:${GITHUB_TOKEN}@github.com/Jonipeloni/musicgeneration.git\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "id": "K96-KfwhAEFY",
        "outputId": "c674e383-48b5-47b7-bc5c-67ab3b2ff26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 4, done.\n",
            "Counting objects:  25% (1/4)\rCounting objects:  50% (2/4)\rCounting objects:  75% (3/4)\rCounting objects: 100% (4/4)\rCounting objects: 100% (4/4), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n",
            "Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 1.86 KiB | 1.86 MiB/s, done.\n",
            "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas:   0% (0/1)\u001b[K\rremote: Resolving deltas: 100% (1/1)\u001b[K\rremote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/Jonipeloni/musicgeneration.git\n",
            "   786b470..7767bc0  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lIPVpuOLB7mG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
