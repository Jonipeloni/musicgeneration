{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jonipeloni/musicgeneration/blob/main/transformer_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3hYoI-NJQO-o"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# from keras_mlp.layers import ReversibleEmbedding\n",
        "\n",
        "class PosEncode(tf.keras.layers.Layer):\n",
        "    def __init__(self,dims,  max_sep_len,):\n",
        "        super(PosEncode, self).__init__()\n",
        "        self.max_sep_len = max_sep_len\n",
        "        self.pos_enc = self._pos_enc(max_sep_len, dims)\n",
        "\n",
        "    #Add the Embeddings and the Encodings\n",
        "\n",
        "    def _pos_enc(self, length, dims):\n",
        "        pos = np.arange(length)[:, np.newaxis]\n",
        "        j = np.arange(dims)[np.newaxis, :]\n",
        "        #distribute the angles according to the formula for Positional Encodings\n",
        "        angle_rates = 1 / np.power(10000, (2 * j)/ dims)\n",
        "        angle_rads = pos * angle_rates\n",
        "        #even coordinates get sin, odd coordinates get cos\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        return tf.slice(self.pos_enc, [0, 0, 0], [-1, seq_len, -1])\n",
        "\n",
        "class InverseEmbedding(tf.keras.layers.Embedding):\n",
        "\n",
        "    def __init__(self, voc_size, dims,  **kwargs):\n",
        "        super(InverseEmbedding, self).__init__(voc_size, dims, **kwargs)\n",
        "\n",
        "    def call(self, x, inverse = False, training = True):\n",
        "        if not inverse:\n",
        "            return super().call(x)\n",
        "\n",
        "        trans = tf.transpose(tf.convert_to_tensor(self.embeddings))\n",
        "        return tf.matmul(x, trans)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dims, num_heads, attn_size, dropout_prob):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Initialize the parameters\n",
        "        self.num_heads = num_heads\n",
        "        self.attn_size = attn_size\n",
        "        self.dims = dims\n",
        "\n",
        "        # Layers for QKV projection and output projection\n",
        "        self.qkv_projection = tf.keras.layers.Dense(num_heads * attn_size * 3, use_bias=False)\n",
        "        self.output_projection = tf.keras.layers.Dense(dims)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_prob)\n",
        "\n",
        "    def call(self, qs, mask=None):\n",
        "        # Compute batch size and sequence length from inputs\n",
        "        batch_size = tf.shape(qs)[0]\n",
        "        seq_length = tf.shape(qs)[1]\n",
        "\n",
        "        # Project queries, keys, and values\n",
        "        qkv = self.qkv_projection(qs)\n",
        "        qkv = tf.reshape(qkv, [batch_size, seq_length, self.num_heads * 3, self.attn_size])\n",
        "        qs, ks, vs = tf.split(qkv, 3, axis=2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        qs = tf.transpose(qs, [0, 2, 1, 3])\n",
        "        ks = tf.transpose(ks, [0, 2, 3, 1])\n",
        "        attn_product = tf.matmul(qs, ks) / tf.math.sqrt(tf.cast(self.attn_size, tf.float32))\n",
        "\n",
        "        # Apply mask, if provided\n",
        "        if mask is not None:\n",
        "            mask = tf.expand_dims(mask, 1)\n",
        "            mask = tf.expand_dims(mask, 2)\n",
        "            mask = tf.broadcast_to(mask, [batch_size, self.num_heads, seq_length, seq_length])\n",
        "            attn_product = tf.where(mask == 0, tf.fill(tf.shape(attn_product), -1e9), attn_product)\n",
        "\n",
        "        # Softmax and dropout\n",
        "        scores = tf.nn.softmax(attn_product, axis=-1)\n",
        "        scores = self.dropout(scores)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        vs = tf.transpose(vs, [0, 2, 1, 3])\n",
        "        res = tf.matmul(scores, vs)\n",
        "\n",
        "        # Reshape and project to output size\n",
        "        res = tf.reshape(tf.transpose(res, [0, 2, 1, 3]), [batch_size, -1, self.num_heads * self.attn_size])\n",
        "        output = self.output_projection(res)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class SubLayerLogic(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dropout_rate):\n",
        "        super(SubLayerLogic, self).__init__()\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def calls(self, x, sublayer):\n",
        "\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class SelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate):\n",
        "        super(SelfAttentionBlock, self).__init__()\n",
        "        self.ffn = tf.keras.layers.Dense(d_model, \"silu\")\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads,d_model//num_heads, dropout_rate)\n",
        "        self.sub1 = SubLayerLogic(d_model, dropout_rate)\n",
        "        self.sub2 = SubLayerLogic(d_model, dropout_rate)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, mask):\n",
        "        x = self.sub1(x, lambda inputs : self.attention(inputs, mask))\n",
        "        return self.sub2(x, self.attention)\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, voc_size, dims, maxseqlen, num_heads, dropout_rate, n_blocks):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.posemb = PosEncode(dims, maxseqlen)\n",
        "        self.embedding = InverseEmbedding(voc_size, dims)\n",
        "        self.blocks = [\n",
        "            SelfAttentionBlock(dims, num_heads, dropout_rate) for _ in range(n_blocks)\n",
        "        ]\n",
        "\n",
        "    def encode(self, x):\n",
        "\n",
        "        return self.posemb(x) + self.embedding(x, inverse=False)\n",
        "\n",
        "    def gen_mask(self, seq_len):\n",
        "        return tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)[None,None,:]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, maks = None):\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        x = self.encode(x)\n",
        "\n",
        "        if mask is None:\n",
        "          mask = self.gen_mask(seq_len)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        return self.embedding(x, inverse = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pm2hgFGBQnag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}