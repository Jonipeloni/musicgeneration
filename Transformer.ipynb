{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jonipeloni/musicgeneration/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhb6vaRs1W3I",
        "outputId": "40866fba-f9b6-445a-d33b-6d04a236ad36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
            "Collecting packaging~=23.1 (from mido>=1.1.16->pretty_midi)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592289 sha256=0418da3f7e5c58df2ba31381898838a87c3d32e638864584af12bc9381159ae6\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/a5/30/7b8b7f58709f5150f67f98fde4b891ebf0be9ef07a8af49f25\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: packaging, mido, pretty_midi\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed mido-1.3.2 packaging-23.2 pretty_midi-0.2.10\n"
          ]
        }
      ],
      "source": [
        "pip install pretty_midi numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pretty_midi\n",
        "\n",
        "# Assuming '/content/drive/My Drive/Midi' contains your MIDI files.\n",
        "midi_folder_path = '/content/drive/My Drive/Midi'\n",
        "\n",
        "def midi_to_tokens(midi_file):\n",
        "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
        "    notes = []\n",
        "    for instrument in midi_data.instruments:\n",
        "        for note in instrument.notes:\n",
        "            notes.append((note.start, note.end, note.pitch, note.velocity))\n",
        "    notes.sort(key=lambda note: note[0])  # Sort by start time\n",
        "    tokens = []\n",
        "    last_end_time = 0\n",
        "    for note in notes:\n",
        "        start_time, end_time, pitch, velocity = note\n",
        "        time_shift = round(start_time - last_end_time, 2)\n",
        "        if time_shift > 0:\n",
        "            tokens.append(f\"TimeShift_{time_shift}\")\n",
        "        tokens.append(f\"NoteOn_{pitch}_{velocity}\")\n",
        "        duration = round(end_time - start_time, 2)\n",
        "        tokens.append(f\"Duration_{duration}\")\n",
        "        last_end_time = end_time\n",
        "    return tokens\n",
        "\n",
        "def process_midi_folder(folder_path):\n",
        "    all_tokens = []\n",
        "    for midi_file in os.listdir(folder_path):\n",
        "        if midi_file.endswith(('.mid', '.midi')):\n",
        "            tokens = midi_to_tokens(os.path.join(folder_path, midi_file))\n",
        "            all_tokens.extend(tokens)\n",
        "    return all_tokens\n",
        "\n",
        "def create_dataset(all_tokens, seq_length=100, batch_size=64):\n",
        "    # Convert tokens to IDs\n",
        "    token_to_id = np.load('/content/drive/My Drive/token_to_id.npy', allow_pickle=True).item()\n",
        "    token_ids = [token_to_id.get(token, token_to_id['UNK']) for token in all_tokens]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(token_ids)\n",
        "    sequences = dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "    def split_input_target(chunk):\n",
        "        input_text = chunk[:-1]\n",
        "        target_text = chunk[1:]\n",
        "        return input_text, target_text\n",
        "\n",
        "    dataset = sequences.map(split_input_target)\n",
        "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "wZINK6dZwHWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "61a6732f-c762-4c33-941a-465179ca9758"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Midi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0abe4eaf655f>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_midi_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmidi_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0abe4eaf655f>\u001b[0m in \u001b[0;36mprocess_midi_folder\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_midi_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmidi_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.midi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmidi_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Midi'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Main execution\n",
        "all_tokens = process_midi_folder(midi_folder_path)\n",
        "dataset = create_dataset(all_tokens)\n"
      ],
      "metadata": {
        "id": "YykUhJlXC-Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'all_tokens' is a list of all tokens you've processed from your MIDI files\n",
        "all_tokens = process_midi_folder(midi_folder_path)  # From your MIDI processing\n",
        "\n",
        "# Count unique tokens and create a mapping to integers\n",
        "token_counts = Counter(all_tokens)\n",
        "unique_tokens = sorted(token_counts.keys())\n",
        "token_to_id = {token: id for id, token in enumerate(unique_tokens, start=1)}\n",
        "\n",
        "# Optionally, include a special token for unknown tokens (if your model needs to handle unseen tokens)\n",
        "token_to_id['UNK'] = len(token_to_id) + 1\n",
        "\n",
        "# Save the mapping for later use\n",
        "np.save('/content/drive/My Drive/token_to_id.npy', token_to_id)\n"
      ],
      "metadata": {
        "id": "ovYHontEwJ9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw3ukTjwD1aQ"
      },
      "outputs": [],
      "source": [
        "#Create the Positional Encodings which are added to the Embeddings\n",
        "def pos_enc(length, d_model):\n",
        "    pos = np.arange(length)[:, np.newaxis]\n",
        "    j = np.arange(d_model)[np.newaxis, :]\n",
        "    #distribute the angles according to the formula for Positional Encodings\n",
        "    angle_rates = 1 / np.power(10000, (2 * (j//2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "    #even coordinates get sin, odd coordinates get cos\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696Y9XQfmFwY"
      },
      "source": [
        "The formulas for Positional Encodings from the original Attention Paper are $$PE_{(pos, 2j)} = \\sin(pos / 10000^{2j / d_{\\text{model}}})$$\n",
        "$$PE_{(pos, 2j+1)} = \\cos(pos / 10000^{2j / d_{\\text{model}}})\n",
        "$$ so we have to implement two formulas, one for the even and one for the odd coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKJaKCEl787M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#Create the Embeddings our Transformer can use\n",
        "class Embedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, voc, d_model, block_length, dropout_rate):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.voc = voc\n",
        "        self.block_length = block_length\n",
        "        self.d_model = d_model\n",
        "        self.emb = tf.keras.layers.Embedding(input_dim=voc,\n",
        "                          output_dim=d_model,\n",
        "                          input_length=block_length)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.pos_enc = pos_enc(block_length, d_model)\n",
        "\n",
        "    #Add the Embeddings and the Encodings\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x = self.emb(x) + self.pos_enc[:, :self.block_length, :]\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBefVncOxv58"
      },
      "outputs": [],
      "source": [
        "#SwigLu\n",
        "class SwiGLu(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, units):\n",
        "        super(SwiGLu, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(units=(units//3)*2, activation=tf.nn.silu, use_bias=False)\n",
        "        self.dense2 = tf.keras.layers.Dense(units=(units//3)*2, use_bias=False)\n",
        "        self.dense3 = tf.keras.layers.Dense(units=d_model, use_bias=False)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        w = self.dense1(x)\n",
        "        v = self.dense2(x)\n",
        "        x = self.dense3(w*v)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6yZw47HZBJu"
      },
      "source": [
        "SwigLu is a special type of Activation Function, where the name Activation Function is actually a bit misleading, because it consists of multiply NN Layers. At first, the input is put into two different Dense Layers having $\\frac{2}{3} \\cdot d_{model}$ units. The outputs of those layers are then multiplied element-wise, and the result is put into a final Dense Layers, so the output dimensionality is $d_{model}$ again. The SiLu Activation Function in the first Dense Layer is defined as $SiLu(x) = x \\cdot Sigmoid(x)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNmyUkNED6qM"
      },
      "outputs": [],
      "source": [
        "#Implement the classic attention layer\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate):\n",
        "        super(Attention, self).__init__()\n",
        "        #Choose head_size in a way that the total dimensionality does not change\n",
        "        self.size_heads = d_model // num_heads\n",
        "        self.query = tf.keras.layers.Dense(units=self.size_heads, use_bias=False)\n",
        "        self.key = tf.keras.layers.Dense(units=self.size_heads, use_bias=False)\n",
        "        self.value = tf.keras.layers.Dense(units=self.size_heads, use_bias=False)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    #Implement the Attention function\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        B, T, C = x.shape\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        #Compute the Attention matrix\n",
        "        scores = tf.matmul(Q,tf.transpose(K, perm=[0, 2, 1])) / tf.math.sqrt(tf.cast(self.size_heads, tf.float32))\n",
        "        #Mask for training\n",
        "        tril   = tf.linalg.band_part(tf.ones((T, T)), -1, 0)\n",
        "        scores = tf.where(tril == 0, tf.fill(tril.shape, -float('inf')), scores)\n",
        "        #Continue Computing the Attention Values\n",
        "        scores = tf.nn.softmax(scores, axis=-1)\n",
        "        scores = self.dropout(scores)\n",
        "        return tf.matmul(scores, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx8rnXLZkAvO"
      },
      "source": [
        "The Attention formula is $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$, where the softmax is applied to each row of the matrix. First, we compute $\\frac{QK^T}{\\sqrt{d_k}}$, then after masking, we put it into the softmax, apply dropout and multiply by V.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkrP9eIhkDqb"
      },
      "outputs": [],
      "source": [
        "#Implement Multi-Head-Attention\n",
        "class MultiHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate):\n",
        "        super(MultiHead, self).__init__()\n",
        "        self.heads = [Attention(d_model, num_heads, dropout_rate) for _ in range(num_heads)]\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model, use_bias=False)\n",
        "    #Write the results in one concatenated matrix to keep the dimensionality\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x = tf.concat([head(x) for head in self.heads], axis=-1)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIEsTGNtyfvr"
      },
      "outputs": [],
      "source": [
        "#Transformer block\n",
        "class Tblock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, units, dropout_rate):\n",
        "        super(Tblock, self).__init__()\n",
        "        self.ffn = SwiGLu(d_model, units)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.attention = MultiHead(d_model, num_heads, dropout_rate)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        res = x\n",
        "        x = self.layernorm1(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.add([res, x])\n",
        "        res = x\n",
        "        x = self.layernorm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.add([res, x])\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22g_BLkezEJU"
      },
      "outputs": [],
      "source": [
        "#Decoder\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, voc, d_model, num_heads, units, dropout_rate, num_layers, block):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(voc, d_model, block, dropout_rate)\n",
        "        self.layers = [Tblock(d_model, num_heads, units, dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1PPNxTfzxnZ"
      },
      "outputs": [],
      "source": [
        "#Full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, voc, d_model, num_heads, units, dropout_rate, num_layers, block):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.decoder = Decoder(voc, d_model, num_heads, units, dropout_rate, num_layers, block)\n",
        "        self.final_dense = tf.keras.layers.Dense(units=voc)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x = self.decoder(x)\n",
        "        logits = self.final_dense(x)\n",
        "        return logits\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC4Qg8b6-Azs"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)  # Ensure d_model is a float\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)  # Cast step to float to avoid type issues\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "transformer = Transformer(\n",
        "    voc=1024,\n",
        "    d_model=512,\n",
        "    num_heads=8,\n",
        "    units=2048,\n",
        "    dropout_rate=0.1,\n",
        "    num_layers=6,\n",
        "    block=100\n",
        ")\n",
        "\n",
        "learning_rate = CustomSchedule(d_model=512)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "#Loss\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "#Training Step\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer(inp, training=True)\n",
        "        loss = loss_function(tar, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "#Training loop\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {train_loss.result()}')\n",
        "    train_loss.reset_states()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LqiVr9zxMA9",
        "outputId": "5855499e-aa7a-439c-9100-edc40551d72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4231685996055603\n",
            "Epoch 2, Loss: 8.616084414825309e-06\n",
            "Epoch 3, Loss: 2.0085099095012993e-07\n",
            "Epoch 4, Loss: 6.788434259163978e-09\n",
            "Epoch 5, Loss: 4.221442639895656e-10\n",
            "Epoch 6, Loss: 1.1678954678351339e-11\n",
            "Epoch 7, Loss: 2.6352518257832802e-12\n",
            "Epoch 8, Loss: 1.976436592512898e-12\n",
            "Epoch 9, Loss: 1.197842416618758e-13\n",
            "Epoch 10, Loss: 0.0\n",
            "Epoch 11, Loss: 0.0\n",
            "Epoch 12, Loss: 0.0\n",
            "Epoch 13, Loss: 0.0\n",
            "Epoch 14, Loss: 0.0\n",
            "Epoch 15, Loss: 1.796763557165501e-13\n",
            "Epoch 16, Loss: 0.0\n",
            "Epoch 17, Loss: 0.0\n",
            "Epoch 18, Loss: 0.0\n",
            "Epoch 19, Loss: 0.0\n",
            "Epoch 20, Loss: 5.989212760720147e-14\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}